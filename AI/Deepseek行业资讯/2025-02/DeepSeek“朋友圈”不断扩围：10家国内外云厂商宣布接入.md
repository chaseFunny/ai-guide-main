# DeepSeek“朋友圈”不断扩围：10家国内外云厂商宣布接入

1月20日，中国AI初创公司深度求索（DeepSeek）推出大模型DeepSeek-R1。作为一款开源模型，R1在数学、代码、自然语言推理等任务上的性能能够比肩OpenAI o1模型正式版，并采用MIT许可协议，支持免费商用、任意修改和衍生开发等。截至2月5日，国内外已有众多云平台宣布上线DeepSeek- R1大模型。

近日，华为云、腾讯云、阿里云、百度智能云等国内主流云平台纷纷宣布上线R1大模型。2月5日，移动云宣布全面上线DeepSeek，实现全版本覆盖、全尺寸适配、全功能畅用。中国移动覆盖全国的13个智算中心全面上线上述能力，用户可选择任一智算资源池进行部署、蒸馏、智能体编排等操作。

同一天，中国联通也宣布，联通云已基于星罗平台实现国产及主流算力适配多规格DeepSeek-R1模型，兼顾私有化和公有化场景，提供全方位运行服务保障。联通云基于A800、H800、L40S等多款主流算力卡，预置DeepSeek-R1多尺寸模型，用户可按需灵活选择、快速扩展，快速搭建DeepSeek-R1推理和微调环境。

此前，2月4日，京东云宣布，言犀AI开发计算平台已支持DeepSeek-V3、DeepSeek-R1以及所有蒸馏小参数模型（DeepSeek-R1-Distill）的一键部署，支持公有云在线部署、专混私有化实例部署两种模式，供用户按需部署，快速调用。

2月4日，字节跳动旗下的火山引擎宣布，将支持 V3/R1 等不同尺寸的 DeepSeek 开源模型，可以通过两种方式进行模型使用：一是在火山引擎机器学习平台 veMLP 中部署，目前 veMLP 已经支持全尺寸 DeepSeek 模型， 并仔细对 SGLang 和 vLLM 做过性能调优和效果评测，火山引擎自研 PD 分离+EP 并行的推理引擎也即将推出，全面开放。适用于自己进行模型定制、部署、推理的企业。二是在火山方舟中调用模型，适用于期望通过 API 快速集成预训练模型的企业，目前已经支持4个模型版本，并提供了全网最高的限流。

2月3日，百度智能云宣布，百度智能云千帆平台已正式上架DeepSeek-R1和 DeepSeek-V3模型，推出了超低价格方案，R1模型输入价格为2元每百万token，输出价格为8元每百万token。用户还可登录百度智能云千帆ModelBuilder享受限时免费服务。

据介绍，百度智能云此次接入的模型已全面融合百度千帆推理链路，集成百度独家内容安全算子，实现模型安全增强与企业级高可用保障，同时支持完善的BLS日志分析（支持查询和分析的API调用日志）和BCM告警（分钟级监控指标告警），助力用户安全、稳定地构建智能应用。

同样是在2月3日，阿里云宣布，阿里云PAI Model Gallery支持云上一键部署DeepSeek-V3、DeepSeek-R1。目前DeepSeek-R1支持采用vLLM加速部署；DeepSeek-V3 支持vLLM加速部署以及Web应用部署；DeepSeek-R1蒸馏小模型支持采用BladeLLM（阿里云PAI自研高性能推理框架）和vLLM加速部署。

2月2日，腾讯云宣布，R1大模型一键部署至腾讯云高性能应用服务HAI上，开发者仅需3分钟就能接入调用。简单来说，通过HAI，开发者可以省去繁琐步骤，只需两步即可调用R1模型，默认加载1.5B参数模型。如果1.5B模型无法满足需求，可在命令行后输入7B/8B/14B等参数指令，自由切换至对应规格的模型。

两天后，腾讯云又宣布推出“开发者大礼包”，DeepSeek全系模型一键部署，从“满血版”671B参数到轻量版1.5B参数都包括在内。腾讯云提供了灵活的付费模式，部署完成后，开发者可在线体验模型效果，还可以获取API调用地址，秒速接入各类AI应用。

2月1日，据华为云官方微信号，硅基流动和华为云团队联合首发并上线基于华为云昇腾云服务的DeepSeek R1/V3推理服务。

公开资料显示，北京硅基流动科技有限公司专注于打造生成式AI计算基础设施平台。据介绍，得益于自研推理加速引擎加持，硅基流动和华为云昇腾云服务支持部署的DeepSeek模型可获得持平全球高端GPU部署模型的效果，同时能够提供稳定的、生产级服务能力，让模型能够在大规模生产环境中稳定运行，并满足业务商用部署需求。

随后，华为还宣布，DeepSeek系列新模型正式上线昇腾社区，支持一键获取DeepSeek系列模型和昇腾硬件平台上开箱即用，且基于原生鸿蒙操作系统的小艺助手App已经接入DeepSeek。

此外，无问芯穹、青云科技、PPIO派欧云、云轴科技等独立云厂商均已宣布适配及上架DeepSeek模型服务。

另一边，海外各大云厂商和芯片厂商在更早之前就已纷纷宣布上线DeepSeek-R1模型。北京时间1月31日，英伟达宣布DeepSeek-R1模型登陆NVIDIA NIM。据介绍，在单个英伟达HGX H200系统上，完整版DeepSeek-R1 671B的处理速度可达每秒3872 Token。英伟达竞争对手AMD也宣布，已将新的DeepSeek-V3模型集成到Instinct MI300X GPU上，针对Al推理进行了优化。

两大云巨头亚马逊AWS和微软Azure也接入了DeepSeek-R1模型。从1月30日起，AWS的用户可以在Amazon Bedrock和Amazon SageMaker AI中部署DeepSeek-R1模型。AWS方面表示：“通过密切关注客户需求和技术进步，AWS定期扩大我们精心挑选的模型选择，以包括有前途的新模型以及既定的行业最爱。高性能和差异化模型产品的持续扩展有助于客户保持在AI创新的最前沿。”

当地时间1月29日，微软宣布DeepSeek-R1已在Azure AI Foundry和GitHub上提供，开发者可以用新模型进行测试和构建基于云的应用程序和服务。微软客户能够使用R1模型的精简版本在微软AI电脑Copilot+PC上本地运行。微软方面表示：“作为Azure AI Foundry的一部分，DeepSeek-R1可以在一个值得信赖的、可扩展的和为企业做好准备的平台上访问，使企业能够无缝集成先进的AI，同时满足SLA（服务水平协议）、安全性和负责任的AI承诺。”

---
来源：澎湃新闻